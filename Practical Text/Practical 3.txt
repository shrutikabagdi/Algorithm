Aim: Apply and evaluate the perceptron model for any particular domain.

Theory:
Introduction:

The perceptron algorithm was inspired by the basic processing units in the brain called neurons and how
they process signals. It was invented by Frank Rosenblatt, using the McCulloch-Pitts neuron and the Findings of Hebb.


A Perceptron Algorithm is not something widely used in practice. e study it mostly for historical reasons and alse because it is the most basic and simple single-layered neural network.

we are going to look at the Perceptron Algorithm, which is the most basic single-layered neural network used for binary classification. First, we will look at the Unit Step Function and see how the Perceptron Algorithm classifies and then have a look at the perceptron update rule.

Finally, we will plot the decision boundary for our data. We will use the data with only two features, and there will be two classes since Perceptron is a binary classifier. We will implement all the code using Python NumPy, and visualize/plot using Matplotlib.


Perceptron Algorithm

The Perceptron is inspired by the information processing of a single neural cell called a neuron.

A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body.

In a similar way, the Perceptron receives input signals from examples of training data that we weight and combine in a linear equation called the activation.

activation = sum(weight * x_i) + bias}

The activation is then transformed into an output value or prediction using a transfer function, such as the step transfer function.

prediction = 1.0 if activation â‰¥ 0.0 else 0.0


In this way, the Perceptron is a classification algorithm for problems with two classes (0 and 1) where a linear equation (like or hyperplane) can be used to separate the two classes.

It is closely related to linear regression and logistic regression that make predictions in a similar way (e.g., a weighted sum of inputs).

The weights of the perceptron algorithm must be estimated from your training data using stochastic gradient descent.





CODE:

import numpy as np 
import tensorflow as tf 
from tensorflow import keras 
import matplotlib.pyplot as plt 
%matplotlib inline 
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() 
x_train 
x_test 
len(x_test) 
len(x_train) 
x_train[0].shape 
plt.matshow(x_train[0]) 
x_train = x_train / 255 
x_test = x_test / 255 
x_train_flatten = x_train.reshape(len(x_train), 28*28) 
x_test_flatten = x_test.reshape(len(x_test), 28*28) 
model = keras.Sequential([ 
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid') 
]) 
model.compile(optimizer='adamax', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy']) 
model.fit(x_train_flatten, y_train, epochs=25) 
model.evaluate(x_test_flatten, y_test) 