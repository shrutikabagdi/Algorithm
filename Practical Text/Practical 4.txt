Aim: Implement Multilayer perceptron by providing different weights.


THEORY: 
Multilayer Perceptron 
The Multilayer Perceptron was developed to tackle this limitation. It is a neural network where the mapping 
between inputs and output is non-linear. 
 
 
A Multilayer Perceptron has input and output layers, and one or more hidden layers with many neurons 
stacked together. And while in the Perceptron the neuron must have an activation function that imposes a 
threshold, like ReLU or sigmoid, neurons in a Multilayer Perceptron can use any arbitrary activation function. 
Multi-Layer Perceptron(MLP) is the simplest type of artificial neural network. It is a combination of multiple 
perceptron models. Perceptrons are inspired by the human brain and try to simulate its functionality to solve                                
problems. In MLP, these perceptrons are highly interconnected and parallel in nature. This parallelization 
helpful in faster computation. 


Perceptron 
Perceptron was introduced by Frank Rosenblatt in 1950. It has the capability to learn complex things just like the human 
brain. Perceptron network consists of three units: Sensory Unit (Input Unit), Associator Unit (Hidden Unit), and 
Response Unit (Output Unit). 
 
 
Multilayer Perceptron falls under the category of feedforward algorithms, because inputs are combined with 
the initial weights in a weighted sum and subjected to the activation function, just like in the Perceptron. But 
the difference is that each linear combination is propagated to the next layer. 

Each layer isfeeding the next one with the result of their computation, their internal representation of the 
data. This goes all the way through the hidden layers to the output layer. 
But it has more to it. 

If the algorithm only computed the weighted sums in each neuron, propagated results to the output layer, and 
stopped there, it wouldn't be able to learn the weights that minimize the cost function. If the algorithm only 
computed one iteration, there would be no actual learning. 
 
How does MLP works? 
The Perceptron consists of an input layer and an output layer which are fully connected. MLPs have the same 
input and output layers but may have multiple hidden layers in between as mentioned in the previous section 
figure. 

 
 
Multi-Layer Perceptron trains model in an iterative manner. In each iteration, partial derivatives of the loss 
function used to update the parameters. We can also use regularization of the loss function to prevent 
overfitting in the model. 

Let's start the model building hands-on in python. 

Load dataset 

Let's first load the required HR dataset using pandas' read CSV function. You can download data from the 
following link: 
import numpy as np 
import pandas as pd 
dat ad   dat ea 

Preprocessing: Label Encoding 
Lots of machine learning algorithms require numerical input data, so you need to represent categorical 
columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. 
Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label 
encoding. In sklearn, we can do this using LabelEncoder. 
 
from sklearn import preprocessing 
le = preprocessing LabelEncod  
dat fit_tr ata  
dat fit_tr ata  

Here, we imported the preprocessing module and created the Label Encoder object. Using this LabelEncoder 
object you fit and transform the “salary” and “Departments “ column into the numeric column. 


Split the dataset 
In order to assess the model performance, we need to divide the dataset into a training set and a test set. Let's 
split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and 
test_set size. 

# Spliting data into Feature and 
X=data[['satisfaction_level', 'last_evaluation', 'number roject', 'average_montly_hours', 
'time_spend_company', 'Work_accident', 'promotion_last_5years', 'Departments ', 'salary']] y=data['left'] 
# Import train_test_split function 
from sklearn.model_selection import train_test_split 
# Split dataset into training set and test set 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 
30% test 

Build Classification Model 

Let's build an employee chum prediction model. Here, our objective is to predict chum using MLPClassifier. 
First, import the MLPClassifier module and create MLP Classifier object using MLPClassifier() function. 
Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). 
from sklearn.neural_network import MLPClassifier # Create model object 
elf= MLPClassifier(hidden_layer_sizes=(6,5), 
random_state=5, verbose=Tru  
leaming_rate_init=0.01) 
# Fit data onto the model 
elf.fit(X_train,y_train) 

Parameters: 
n  hidden_layer_sizes: it is a tuple where each element represents one layer and its value represents the 
number of neurons on each hidden layer. 
learnin rate_init: It used to controls the step-size in updating the weights. 
activation: Activation function for the hidden layer. Examples, identity, logistic, tanh, and relu. by 
default, relu is used as an activation function. 
u random_state: It defines the random number for weights and bias initialization. 
■ verbose: It used to print progress messages to standard output. 

Make Prediction and Evaluate the Model 
In this section, we will make predictions on the test dataset and assess model accuracy based on available 
actual labels of the test dataset. 





CODE:

from google.colab import drive 
drive.mount('/content/drive') 
import numpy as np 
import pandas as pd 
path="/content/HR_comma_sep - HR_comma_sep.csv" 
df=pd.read_csv(path) 
df.head(5) 
df.shape 
df.isnull().sum() 
df.info() 
df.describe() 
df.tail() 
x = df.drop('left', axis=1) 
y = df['left'] 
from sklearn.preprocessing import LabelEncoder 
le = LabelEncoder() 
x_encoded = x.copy() 
x_encoded['Department'] = le.fit_transform(x_encoded['Department']) 
x_encoded['salary'] = le.fit_transform(x_encoded['salary']) 
from sklearn.model_selection import train_test_split 
x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.2, random_state=42) 
from sklearn.neural_network import MLPClassifier 
mlp=MLPClassifier(hidden_layer_sizes=(6,5),random_state=5,verbose=True,learning_rate_init=0.01) 
mlp.fit(x_train, y_train) 
from sklearn.metrics import accuracy_score 
y_pred = mlp.predict(x_test) 
accuracy = accuracy_score(y_test, y_pred) 
print(f"Accuracy: {accuracy}") 