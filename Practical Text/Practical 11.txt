Aim: Perform and implement LSTM (Long Short-Term Memory) on IMDb dataset. 

THEORY:

THEORY: 
Long Short-Term Memory Networks or LSTM in deep learning, is a sequential neural network that allows 
information to persist. It is a special type of Recurrent Neural Network which is capable of handling the 
vanishing gradient problem faced by RNN. LSTM was designed by Hochreiter and Schmidhuber that 
resolves the problem caused by traditional rnns and machine learning algorithms 
 
LSTM (Long Short-Term Memory) is a recurrent neural network (RNN) architecture widely used in 
Deep Learning. It excels at capturing long-term dependencies, making it ideal for sequence prediction 
tasks. Unlike traditional neural networks, LSTM incorporates feedback connections, allowing it to 
process entire sequences of data, not just individual data points. This makes it highly effective in 
understanding and predicting patterns in sequential data like time series, text, and speech. LSTM has 
become a powerful tool in artificial intelligence and deep learning, enabling breakthroughs in various fields 
by uncovering valuable insights from sequential data. 

LSTM Architecture: 
In the introduction to long short-term memory, we learned that it resolves the vanishing gradient 
problem faced by RNN, so now, in this section, we will see how it resolves this problem by learning 
the architecture of the LSTM. At a high level, LSTM works very much like an RNN cell. Here is the internal 
functioning of the LSTM network. The LSTM network architecture consists of three parts, as shown 
in the image below, and each part performs an individual function. 

 
The Logic Behind LSTM: 
The first part chooses whether the information coming from the previous timestamp is to be 
remembered or is irrelevant and can be forgotten. In the second part, the cell tries to learn new information 
from the input to this cell. At last, in the third part, the cell passes the updated information from the 
current timestamp to the next timestamp. This one cycle of LSTM is considered a single-time step. 
These three parts of an LSTM unit are known as gates. They control the flow of information in and 
out of the memory cell or lstm cell. The first gate is called Forget gate, the second gate is known as 
the Input gate, and the last one is the Output gate. An LSTM unit that consists of these three gates 
and a memory cell or lstm cell can be considered as a layer of neurons in traditional feedforward 
neural network, with each neuron having a hidden layer and a current state. 
 
 
Just like a simple RNN, an LSTM also has a hidden state where H(t-1) represents the hidden state of 
the previous timestamp and Ht is the hidden state of the current timestamp. In addition to that, LSTM 
also has a cell state represented by C(t-1) and C(t) for the previous and current timestamps, respectively. 
 
Here the hidden state is known as Short term memory, and the cell state is known as Long term 
memory. Refer to the following image. 
 
 
It is interesting to note that the cell state carries the information along with all the timestamps. 
 
Example of LTSM Working: 
 
Let’s take an example to understand how LSTM works. Here we have two sentences separated by a 
full stop. The first sentence is “Bob is a nice person,” and the second sentence is “Dan, on the Other 
hand, is evil”. It is very clear, in the first sentence, we are talking about Bob, and as soon as we 
encounter the full stop(.), we started talking about Dan. 
As we move from the first sentence to the second sentence, our network should realize that we are no 
more talking about Bob. Now our subject is Dan. Here, the Forget gate of the network allows it to 
forget about it. Let’s understand the roles played by these gates in LSTM architecture. 
 
Forget Gate 
In a cell of the LSTM neural network, the first step is to decide whether we should keep the 
information from the previous time step or forget it. Here is the equation for forget gate. 

Let’s try to understand the equation, here 
• Xt: input to the current timestamp. 
• Uf: weight associated with the input 
• Ht-1: The hidden state of the previous timestamp 
• Wf: It is the weight matrix associated with the hidden state 
Later, a sigmoid function is applied to it. That will make ft a number between 0 and 1. This ft is later 
multiplied with the cell state of the previous timestamp, as shown below. 
 
Input Gate 
Let’s take another example. 

 
 
“Bob knows swimming. He told me over the phone that he had served the navy for four long years.” 
So, in both these sentences, we are talking about Bob. However, both give different kinds of 
information about Bob. In the first sentence, we get the information that he knows swimming. 
Whereas the second sentence tells, he uses the phone and served in the navy for four years. 
 
Now just think about it, based on the context given in the first sentence, which information in the 
second sentence is critical? First, he used the phone to tell, or he served in the navy. In this context, 
it doesn’t matter whether he used the phone or any other medium of communication to pass on the 
information. The fact that he was in the navy is important information, and this is something we want 
our model to remember for future computation. This is the task of the Input gate. 
The input gate is used to quantify the importance of the new information carried by the input. Here is 
the equation of the input gate 
 
Here, 
• Xt: Input at the current timestamp t 
• Ui: weight matrix of input 
• Ht-1: A hidden state at the previous timestamp 
• Wi: Weight matrix of input associated with hidden state 
Again we have applied the sigmoid function over it. As a result, the value of I at timestamp t will be 
between 0 and 1.
 
New Information 
 
Now the new information that needed to be passed to the cell state is a function of a hidden state at 
the previous timestamp t-1 and input x at timestamp t. The activation function here is tanh. Due to 
the tanh function, the value of new information will be between -1 and 1. If the value of Nt is 
negative, the information is subtracted from the cell state, and if the value is positive, the 
information is added to the cell state at the current timestamp. 
 
However, the Nt won’t be added directly to the cell state. Here comes the updated equation: 
 
 
Here, Ct-1 is the cell state at the current timestamp, and the others are the values we have calculated 
previously.

 
Output Gate 
 
Now consider this sentence. 
 
“Bob single-handedly fought the enemy and died for his country. For his contributions, 
brave .” 
During this task, we have to complete the second sentence. Now, the minute we see the word brave, 
we know that we are talking about a person. In the sentence, only Bob is brave, we can not say the 
enemy is brave, or the country is brave. So based on the current expectation, we have to give a 
relevant word to fill in the blank. That word is our output, and this is the function of our Output gate. 
Here is the equation of the Output gate, which is pretty similar to the two previous gates. 
 
Its value will also lie between 0 and 1 because of this sigmoid function. Now to calculate the current 
hidden state, we will use Ot and tanh of the updated cell state. As shown below. 
 
It turns out that the hidden state is a function of Long term memory (Ct) and the current output. If 
you need to take the output of the current timestamp, just apply the SoftMax activation on hidden state 
Ht. 
 
Here the token with the maximum score in the output is the prediction. 
This is the More intuitive diagram of the LSTM network. 
 
 
Advantages: 
1. Handles long-term dependencies effectively compared to simple RNNs. 
2. Reduces the problem of vanishing and exploding gradients. 
3. Suitable for sequential data like text, audio, and time series. 
4. Can remember relevant information for long durations. 
5. Provides better accuracy in natural language processing tasks such as sentiment analysis. 
 
Disadvantages: 
1. Computationally more expensive than simple RNNs due to complex architecture. 
2. Requires large datasets and high processing power for training. 
3. Training time is longer because of multiple gate mechanisms. 
4. Difficult to interpret the internal workings (less explainable). 
5. May still struggle with extremely long sequences compared to Transformers. 
 
Applications: 
1. Sentiment Analysis: Classifying text as positive or negative (e.g., IMDb movie reviews). 
2. Speech Recognition: Understanding and transcribing spoken language. 
3. Text Generation: Generating text sequences such as poetry, code, or chat responses. 
4. Machine Translation: Translating text from one language to another. 
5. Stock Price Prediction: Forecasting time-series data based on historical patterns. 
6. Music Composition: Generating new melodies based on learned patterns. 



CODE:

# Step 1: Import libraries 
import numpy as np 
import tensorflow as tf 
from tensorflow.keras import layers, models, datasets, preprocessing 
# Step 2: Load IMDb dataset 
# Keep only top 10,000 most frequent words 
num_words = 10000 
maxlen = 200 # Cut off reviews longer than 200 words 
(x_train, y_train), (x_test, y_test)=datasets.imdb.load_data (num_words=num_words) 
# Step 3: Pad sequences (make all reviews same length) 
x_train = preprocessing.sequence.pad_sequences (x_train, maxlen=maxlen) 
x_test =  preprocessing.sequence.pad_sequences (x_test, maxlen=maxlen) 
print("Training data shape:", x_train.shape) 
print("Test data shape:", x_test.shape) 
model = models. Sequential([ 
  layers.Embedding (input_dim=num_words, output_dim=128, input_length=maxlen), 
  layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2), 
  layers.Dense (1, activation='sigmoid') 
]) 
# Step 5: Compile model 
model.compile(loss='binary_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']) 
model.summary() 
# Step 6: Train model 
history = model.fit(x_train, y_train, 
                    batch_size=64, 
                    epochs = 5, 
                    validation_split=0.2, 
                    verbose = 1) 
#Step 7: Evaluate model 
loss, accuracy = model.evaluate(x_test, y_test, verbose=1) 
print("Test loss:", loss) 
print("Test accuracy:", accuracy)